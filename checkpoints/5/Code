python databricks

Command 1:

# reference: https://colab.research.google.com/drive/1qa14mieeSNYbIpmu4_kcf91_GHKltmDM#scrollTo=f23u6zQC2ZGy
# HEAVILY references: https://developers.google.com/machine-learning/guides/text-classification/

import tensorflow as tf
from tensorflow import keras
import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

from tensorflow.python.keras import models
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.layers import Dropout


Command 2:

# get input data as a dataframe
cases_pd_df = pd.read_csv('/dbfs/FileStore/tables/cases_narratives.csv')
#cases_pd_df


Command 3:

from sklearn.model_selection import train_test_split

# create stratified test/train split 
(xtrain, xtest, ytrain, ytest) = train_test_split(data, labels, test_size=0.2, stratify = labels)

# verify stratification occurred correctly
ytrain.groupby(ytrain).size() / ytrain.count()
# ytest.groupby(ytest).size() / ytest.count()


Command 4:

# Step to validate data and help choose model type (https://developers.google.com/machine-learning/guides/text-classification/step-2-5)
def get_num_words_per_sample(sample_texts):
    """Returns the median number of words per sample given corpus.

    # Arguments
        sample_texts: list, sample texts.

    # Returns
        int, median number of words per sample.
    """
    num_words = [len(s.split()) for s in sample_texts]
    return np.median(num_words)
  
data.size / get_num_words_per_sample(data)


Command 5:

# Encode labels
labels_dict = {'WHITE': 0, 'BLACK': 1, 'WHITE HISPANIC': 2, 'ASIAN/PACIFIC ISLANDER': 3, 'AMER IND/ALASKAN NATIVE': 4}
ytrain_encoded = ytrain.replace(labels_dict)
ytest_encoded = ytest.replace(labels_dict)


Command 6:

# Vectorization parameters
# Range (inclusive) of n-gram sizes for tokenizing text.
NGRAM_RANGE = (1, 2)

# Limit on the number of features. We use the top 20K features.
TOP_K = 20000

# Whether text should be split into word or character n-grams.
# One of 'word', 'char'.
TOKEN_MODE = 'word'

# Minimum document/corpus frequency below which a token will be discarded.
MIN_DOCUMENT_FREQUENCY = 2

def ngram_vectorize(train_texts, train_labels, test_texts):
    """Vectorizes texts as n-gram vectors.

    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.

    # Arguments
        train_texts: list, training text strings.
        train_labels: np.ndarray, training labels.
        test_texts: list, test text strings.

    # Returns
        x_train, x_test: vectorized training and test texts
    """
    # Create keyword arguments to pass to the 'tf-idf' vectorizer.
    kwargs = {
            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.
            'dtype': 'int32',
            'strip_accents': 'unicode',
            'decode_error': 'replace',
            'analyzer': TOKEN_MODE,  # Split text into word tokens.
            'min_df': MIN_DOCUMENT_FREQUENCY,
    }
    vectorizer = TfidfVectorizer(**kwargs)

    # Learn vocabulary from training texts and vectorize training texts.
    x_train = vectorizer.fit_transform(train_texts)

    # Vectorize validation texts.
    x_test = vectorizer.transform(test_texts)

    # Select top 'k' of the vectorized features.
    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))
    selector.fit(x_train, train_labels)
    x_train = selector.transform(x_train).astype('float32')
    x_test = selector.transform(x_test).astype('float32')
    return x_train, x_test
  

def mlp_model(layers, units, dropout_rate, input_shape, num_classes):
    """Creates an instance of a multi-layer perceptron model.

    # Arguments
        layers: int, number of `Dense` layers in the model.
        units: int, output dimension of the layers.
        dropout_rate: float, percentage of input to drop at Dropout layers.
        input_shape: tuple, shape of input to the model.
        num_classes: int, number of output classes.

    # Returns
        An MLP model instance.
    """
    model = models.Sequential()
    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))

    for _ in range(layers-1):
        model.add(Dense(units=units, activation='relu'))
        model.add(Dropout(rate=dropout_rate))

    model.add(Dense(units=5, activation='softmax'))
    return model

  
def train_ngram_model(train_texts, train_labels, test_texts, test_labels,
                      learning_rate=1e-3,
                      epochs=1000,
                      batch_size=128,
                      layers=2,
                      units=64,
                      dropout_rate=0.2):
    """Trains n-gram model on the given dataset.

    # Arguments
        learning_rate: float, learning rate for training model.
        epochs: int, number of epochs.
        batch_size: int, number of samples per batch.
        layers: int, number of `Dense` layers in the model.
        units: int, output dimension of Dense layers in the model.
        dropout_rate: float: percentage of input to drop at Dropout layers.

    """

    # Vectorize texts.
    x_train, x_test = ngram_vectorize(train_texts, train_labels, test_texts)

    # Create model instance.
    model = mlp_model(layers=layers,
                      units=units,
                      dropout_rate=dropout_rate,
                      input_shape=x_train.shape[1:],
                      num_classes=5)

    # Compile model with learning parameters.
    loss = 'sparse_categorical_crossentropy'
    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)
    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])

    # Create callback for early stopping on validation loss. If the loss does
    # not decrease in two consecutive tries, stop training.
    callbacks = [tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=2)]

    # Train and validate model.
    history = model.fit(
            x_train,
            train_labels,
            epochs=epochs,
            callbacks=callbacks,
            validation_split=0.1,
            #validation_data=(x_val, val_labels),
            verbose=2,  # Logs once per epoch.
            batch_size=batch_size)

    # Print results.
    print('\nModel evaluation:')
    score = model.evaluate(x_test,test_labels)
    print('Accuracy: {}'.format(score[1]))

    return model
    
    
    
Command 7:

# Train model
model = train_ngram_model(xtrain, ytrain_encoded, xtest, ytest_encoded)


Command 8:

# Tune hyperparameters
model = train_ngram_model(xtrain, ytrain_encoded, xtest, ytest_encoded, layers=4, units=32)

